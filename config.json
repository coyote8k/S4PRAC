Practical 1: Introduction to TensorFlow

##########################
A)	Performing matrix multiplication and finding eigenvectors and eigenvalues using TensorFlow.
•	Create tensors with different shapes and data types.
•	Perform basic operations like addition, subtraction, multiplication, and division on tensors.
•	Reshape, slice, and index tensors to extract specific elements or sections.

Code:
import tensorflow as tf

# Step 1: Create tensors with different shapes and data types
tensor_float32 = tf.constant([[1.5, 2.5], [3.5, 4.5]], dtype=tf.float32)
tensor_int32 = tf.constant([[1, 2], [3, 4]], dtype=tf.int32)

# Convert tensor_int32 to float32 to avoid type mismatch
tensor_int32 = tf.cast(tensor_int32, tf.float32)

# Step 2: Perform basic operations on tensors
tensor_add = tensor_float32 + tensor_int32
tensor_sub = tensor_float32 - tensor_int32
tensor_mul = tensor_float32 * tensor_int32
tensor_div = tensor_float32 / tensor_int32

# Step 3: Reshape, slice, and index tensors
reshaped_tensor = tf.reshape(tensor_float32, (1, 4))
slice_tensor = tensor_float32[0, 1]
section_tensor = tensor_float32[:, 1]

# Step 4: Perform matrix multiplication
matrix_a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)
matrix_b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)
matrix_multiply = tf.matmul(matrix_a, matrix_b)

# Step 5: Find eigenvectors and eigenvalues
eigenvalues, eigenvectors = tf.linalg.eigh(matrix_a)

# Print results
print("Addition:\n", tensor_add.numpy())
print("Subtraction:\n", tensor_sub.numpy())
print("Multiplication:\n", tensor_mul.numpy())
print("Division:\n", tensor_div.numpy())
print("Reshaped tensor:\n", reshaped_tensor.numpy())
print("Sliced tensor (element at [0, 1]):", slice_tensor.numpy())
print("Extracted section (second column):", section_tensor.numpy())
print("Matrix multiplication result:\n", matrix_multiply.numpy())
print("Eigenvalues:\n", eigenvalues.numpy())
print("Eigenvectors:\n", eigenvectors.numpy())

Output:
Addition:
 [[2.5 4.5]
 [6.5 8.5]]
Subtraction:
 [[0.5 0.5]
 [0.5 0.5]]
Multiplication:
 [[ 1.5  5. ]
 [10.5 18. ]]
Division:
 [[1.5       1.25     ]
 [1.1666666 1.125    ]]
Reshaped tensor:
 [[1.5 2.5 3.5 4.5]]
Sliced tensor (element at [0, 1]): 2.5
Extracted section (second column): [2.5 4.5]
Matrix multiplication result:
 [[19. 22.]
 [43. 50.]]
Eigenvalues:
 [-0.8541019  5.854102 ]
Eigenvectors:
 [[ 0.8506508  0.5257311]
 [-0.5257311  0.8506508]]

########################################
 
B)	Program to solve the XOR problem.
Code:
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import Input
import numpy as np

# Step 1: Prepare the XOR dataset
# Input data (X)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

# Output data (Y) for XOR
Y = np.array([[0], [1], [1], [0]])

# Step 2: Define the neural network model
model = Sequential()

# Add input layer (2 input nodes) and first hidden layer with 4 neurons and ReLU activation
model.add(Input(shape=(2,)))
model.add(Dense(4, activation='relu'))

# Add output layer with 1 neuron and sigmoid activation function (for binary output)
model.add(Dense(1, activation='sigmoid'))

# Step 3: Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Step 4: Train the model
model.fit(X, Y, epochs=10000, verbose=0)

# Step 5: Evaluate the model
loss, accuracy = model.evaluate(X, Y)
print(f'Accuracy: {accuracy*100:.2f}%')

# Step 6: Make predictions
predictions = model.predict(X)
print("\nPredictions on XOR data:")
for i in range(len(X)):
    print(f"Input: {X[i]} - Predicted Output: {predictions[i][0]:.4f}, Actual Output: {Y[i][0]}")
Output:
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 298ms/step - accuracy: 1.0000 - loss: 6.0945e-04
Accuracy: 100.00%
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 146ms/step

Predictions on XOR data:
Input: [0 0] - Predicted Output: 0.0020, Actual Output: 0
Input: [0 1] - Predicted Output: 0.9998, Actual Output: 1
Input: [1 0] - Predicted Output: 0.9998, Actual Output: 1
Input: [1 1] - Predicted Output: 0.0002, Actual Output: 0

--------------------------------------------------------------------------
 
Practical 2: Linear Regression
•	Implement a simple linear regression model using TensorFlow's low-level API (or tf.keras).
•	Train the model on a toy dataset (e.g., housing prices vs. square footage).
•	Visualize the loss function and the learned linear relationship.
Make predictions on new data points.

Code:
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Create a toy dataset (square footage vs. housing prices)
np.random.seed(42)
X_train = np.random.rand(100, 1) * 1000  # Square footage (0 to 1000 sqft)
y_train = 200 + 0.5 * X_train + np.random.randn(100, 1) * 50  # Price = 200 + 0.5 * sqft + noise

# Step 2: Build the linear regression model using tf.keras with ReLU activation for the output layer
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, input_dim=1, activation='linear'),  # Linear layer for the first layer
    tf.keras.layers.ReLU()  # Apply ReLU activation to output layer to ensure positive prices
])

# Step 3: Compile the model (using Mean Squared Error loss and Adam optimizer)
model.compile(optimizer='adam', loss='mse')

# Step 4: Train the model
history = model.fit(X_train, y_train, epochs=100, verbose=0)

# Step 5: Visualize the loss function
plt.plot(history.history['loss'])
plt.title('Loss Function over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss (Mean Squared Error)')
plt.show()

# Step 6: Visualize the learned linear relationship
plt.scatter(X_train, y_train, label="Data")
plt.plot(X_train, model.predict(X_train), color='red', label="Fitted line")
plt.xlabel('Square Footage')
plt.ylabel('Price')
plt.title('Learned Linear Relationship')
plt.legend()
plt.show()

# Step 7: Make predictions on new data points
new_square_footage = np.array([[1500], [2000], [2500]])  # New data points (sqft)
predictions = model.predict(new_square_footage)

# Display predictions
for sqft, price in zip(new_square_footage, predictions):
    print(f"Predicted price for {sqft[0]} sqft: ${price[0]:.2f}")

Output:
 
4/4 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step
 
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 90ms/step
Predicted price for 1500 sqft: $1237.64
Predicted price for 2000 sqft: $1650.15
Predicted price for 2500 sqft: $2062.67

-------------------------------------------------------------------------------------------
 
Practical 3: Convolutional Neural Networks (Classification)
A)	Implementing deep neural network for performing binary classification task.
Code:
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
import numpy as np
import os

# Generate synthetic binary classification dataset (e.g., using random noise as an example)
def create_synthetic_data(num_samples=1000, image_size=(64, 64)):
    np.random.seed(42)
    X = np.random.rand(num_samples, *image_size, 3)
    y = np.random.randint(0, 2, num_samples)  # Binary labels (0 or 1)
    return X, y

# Step 1: Create dataset
image_size = (64, 64)  # Image dimensions
num_samples = 1000
X, y = create_synthetic_data(num_samples, image_size)

# Step 2: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Build the CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(*image_size, 3)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Step 4: Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Step 5: Train the model
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.2
)

# Step 6: Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Step 7: Save the model
model.save("binary_classification_cnn.h5")
print("Model saved successfully.")

Output:
Epoch 1/10
/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
20/20 ━━━━━━━━━━━━━━━━━━━━ 5s 56ms/step - accuracy: 0.5418 - loss: 0.8025 - val_accuracy: 0.4437 - val_loss: 0.6936
Epoch 2/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.4848 - loss: 0.6934 - val_accuracy: 0.4437 - val_loss: 0.7017
Epoch 3/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5455 - loss: 0.6910 - val_accuracy: 0.4437 - val_loss: 0.6969
Epoch 4/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5154 - loss: 0.6927 - val_accuracy: 0.4437 - val_loss: 0.7037
Epoch 5/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5516 - loss: 0.6881 - val_accuracy: 0.4437 - val_loss: 0.7010
Epoch 6/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5307 - loss: 0.6906 - val_accuracy: 0.4437 - val_loss: 0.7003
Epoch 7/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5344 - loss: 0.6901 - val_accuracy: 0.4437 - val_loss: 0.7012
Epoch 8/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5294 - loss: 0.6891 - val_accuracy: 0.4437 - val_loss: 0.6998
Epoch 9/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.5364 - loss: 0.6863 - val_accuracy: 0.4437 - val_loss: 0.7106
Epoch 10/10
20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.5351 - loss: 0.6820 - val_accuracy: 0.4250 - val_loss: 0.6963
7/7 ━━━━━━━━━━━━━━━━━━━━ 1s 87ms/step - accuracy: 0.5297 - loss: 0.6927
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
Test Loss: 0.6935
Test Accuracy: 0.4950
Model saved successfully.

###############################
 
B)	Using a deep feed-forward network with two hidden layers for performing multiclass classification and predicting the class.
Code:
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from tensorflow.keras.utils import to_categorical

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the pixel values
x_train = x_train / 255.0
x_test = x_test / 255.0

# One-hot encoding of labels
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

dnn_model = Sequential()
dnn_model.add(Flatten(input_shape=(28, 28)))
dnn_model.add(Dense(128, activation='relu'))
dnn_model.add(Dense(64, activation='relu'))
dnn_model.add(Dense(10, activation='softmax'))

dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

dnn_model.fit(x_train, y_train_cat, epochs=10, batch_size=128, validation_split=0.2)

dnn_loss, dnn_acc = dnn_model.evaluate(x_test, y_test_cat)
print("DNN Test Accuracy:", dnn_acc)

x_train_cnn = x_train.reshape(-1, 28, 28, 1)
x_test_cnn = x_test.reshape(-1, 28, 28, 1)

cnn_model = Sequential()
cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
cnn_model.add(MaxPooling2D(pool_size=(2, 2)))
cnn_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
cnn_model.add(MaxPooling2D(pool_size=(2, 2)))
cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='relu'))
cnn_model.add(Dense(10, activation='softmax'))

cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

cnn_model.fit(x_train_cnn, y_train_cat, epochs=10, batch_size=128, validation_split=0.2)

cnn_loss, cnn_acc = cnn_model.evaluate(x_test_cnn, y_test_cat)
print("CNN Test Accuracy:", cnn_acc)

print(f"DNN Accuracy: {dnn_acc * 100:.2f}%")
print(f"CNN Accuracy: {cnn_acc * 100:.2f}%")

Output:
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 2s 0us/step
/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Epoch 1/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 4s 5ms/step - accuracy: 0.8089 - loss: 0.7009 - val_accuracy: 0.9455 - val_loss: 0.1932
Epoch 2/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9528 - loss: 0.1652 - val_accuracy: 0.9588 - val_loss: 0.1390
Epoch 3/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9691 - loss: 0.1070 - val_accuracy: 0.9641 - val_loss: 0.1157
Epoch 4/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9765 - loss: 0.0787 - val_accuracy: 0.9690 - val_loss: 0.1035
Epoch 5/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - accuracy: 0.9820 - loss: 0.0604 - val_accuracy: 0.9703 - val_loss: 0.0974
Epoch 6/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 1s 4ms/step - accuracy: 0.9854 - loss: 0.0465 - val_accuracy: 0.9736 - val_loss: 0.0922
Epoch 7/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9890 - loss: 0.0379 - val_accuracy: 0.9730 - val_loss: 0.0903
Epoch 8/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9922 - loss: 0.0279 - val_accuracy: 0.9740 - val_loss: 0.0891
Epoch 9/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9938 - loss: 0.0227 - val_accuracy: 0.9737 - val_loss: 0.0931
Epoch 10/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9944 - loss: 0.0201 - val_accuracy: 0.9754 - val_loss: 0.0901
313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step - accuracy: 0.9739 - loss: 0.0931
DNN Test Accuracy: 0.9779999852180481
Epoch 1/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 6s 9ms/step - accuracy: 0.8517 - loss: 0.5190 - val_accuracy: 0.9766 - val_loss: 0.0779
Epoch 2/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9798 - loss: 0.0684 - val_accuracy: 0.9818 - val_loss: 0.0623
Epoch 3/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9854 - loss: 0.0460 - val_accuracy: 0.9866 - val_loss: 0.0460
Epoch 4/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9884 - loss: 0.0367 - val_accuracy: 0.9854 - val_loss: 0.0520
Epoch 5/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9917 - loss: 0.0261 - val_accuracy: 0.9839 - val_loss: 0.0542
Epoch 6/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9932 - loss: 0.0217 - val_accuracy: 0.9901 - val_loss: 0.0362
Epoch 7/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9949 - loss: 0.0163 - val_accuracy: 0.9866 - val_loss: 0.0491
Epoch 8/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.9948 - loss: 0.0159 - val_accuracy: 0.9898 - val_loss: 0.0370
Epoch 9/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 3s 5ms/step - accuracy: 0.9963 - loss: 0.0111 - val_accuracy: 0.9895 - val_loss: 0.0376
Epoch 10/10
375/375 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - accuracy: 0.9971 - loss: 0.0092 - val_accuracy: 0.9873 - val_loss: 0.0517
313/313 ━━━━━━━━━━━━━━━━━━━━ 2s 4ms/step - accuracy: 0.9854 - loss: 0.0529
CNN Test Accuracy: 0.9882000088691711
DNN Accuracy: 97.80%
CNN Accuracy: 98.82%

-----------------------------------------------
 
Practical 4: Write a program to implement deep learning Techniques for image segmentation.
Code:
import tensorflow as tf
from tensorflow.keras import layers, models

# Define the U-Net model
def unet_model(input_size=(128, 128, 3)):
    inputs = layers.Input(input_size)

    # Encoder
    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)
    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)
    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)
    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)
    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)
    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)
    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)
    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)
    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)

    # Bottleneck
    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)
    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)

    # Decoder
    up6 = layers.Conv2D(512, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv5))
    merge6 = layers.concatenate([conv4, up6], axis=3)
    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(merge6)
    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)

    up7 = layers.Conv2D(256, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv6))
    merge7 = layers.concatenate([conv3, up7], axis=3)
    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(merge7)
    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)

    up8 = layers.Conv2D(128, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv7))
    merge8 = layers.concatenate([conv2, up8], axis=3)
    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(merge8)
    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)

    up9 = layers.Conv2D(64, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(conv8))
    merge9 = layers.concatenate([conv1, up9], axis=3)
    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge9)
    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)
    conv9 = layers.Conv2D(2, 3, activation='relu', padding='same')(conv9)

    conv10 = layers.Conv2D(1, 1, activation='sigmoid')(conv9)

    model = models.Model(inputs=inputs, outputs=conv10)

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Create the model
model = unet_model()

# Summary of the model
model.summary()

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define data generators
data_gen_args = dict(rescale=1./255)
image_datagen = ImageDataGenerator(**data_gen_args)
mask_datagen = ImageDataGenerator(**data_gen_args)

# Provide the paths to your images and masks
image_generator = image_datagen.flow_from_directory(
    'path_to_images',
    class_mode=None,
    target_size=(128, 128),
    batch_size=32,
    seed=42)

mask_generator = mask_datagen.flow_from_directory(
    'path_to_masks',
    class_mode=None,
    color_mode='grayscale',
    target_size=(128, 128),
    batch_size=32,
    seed=42)

def combined_generator(image_gen, mask_gen):
    while True:
        image = next(image_gen)
        mask = next(mask_gen)
        yield image, mask

train_generator = combined_generator(image_generator, mask_generator)

# Train the model
model.fit(train_generator, steps_per_epoch=200, epochs=50)

Output:
Model: "functional_23"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input_layer_13      │ (None, 128, 128,  │          0 │ -                 │
│ (InputLayer)        │ 3)                │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_125 (Conv2D) │ (None, 128, 128,  │      1,792 │ input_layer_13[0… │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_126 (Conv2D) │ (None, 128, 128,  │     36,928 │ conv2d_125[0][0]  │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_25    │ (None, 64, 64,    │          0 │ conv2d_126[0][0]  │
│ (MaxPooling2D)      │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_127 (Conv2D) │ (None, 64, 64,    │     73,856 │ max_pooling2d_25… │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_128 (Conv2D) │ (None, 64, 64,    │    147,584 │ conv2d_127[0][0]  │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_26    │ (None, 32, 32,    │          0 │ conv2d_128[0][0]  │
│ (MaxPooling2D)      │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_129 (Conv2D) │ (None, 32, 32,    │    295,168 │ max_pooling2d_26… │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_130 (Conv2D) │ (None, 32, 32,    │    590,080 │ conv2d_129[0][0]  │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_27    │ (None, 16, 16,    │          0 │ conv2d_130[0][0]  │
│ (MaxPooling2D)      │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_131 (Conv2D) │ (None, 16, 16,    │  1,180,160 │ max_pooling2d_27… │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_132 (Conv2D) │ (None, 16, 16,    │  2,359,808 │ conv2d_131[0][0]  │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ max_pooling2d_28    │ (None, 8, 8, 512) │          0 │ conv2d_132[0][0]  │
│ (MaxPooling2D)      │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_133 (Conv2D) │ (None, 8, 8,      │  4,719,616 │ max_pooling2d_28… │
│                     │ 1024)             │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_134 (Conv2D) │ (None, 8, 8,      │  9,438,208 │ conv2d_133[0][0]  │
│                     │ 1024)             │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ up_sampling2d_20    │ (None, 16, 16,    │          0 │ conv2d_134[0][0]  │
│ (UpSampling2D)      │ 1024)             │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_135 (Conv2D) │ (None, 16, 16,    │  2,097,664 │ up_sampling2d_20… │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_20      │ (None, 16, 16,    │          0 │ conv2d_132[0][0], │
│ (Concatenate)       │ 1024)             │            │ conv2d_135[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_136 (Conv2D) │ (None, 16, 16,    │  4,719,104 │ concatenate_20[0… │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_137 (Conv2D) │ (None, 16, 16,    │  2,359,808 │ conv2d_136[0][0]  │
│                     │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ up_sampling2d_21    │ (None, 32, 32,    │          0 │ conv2d_137[0][0]  │
│ (UpSampling2D)      │ 512)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_138 (Conv2D) │ (None, 32, 32,    │    524,544 │ up_sampling2d_21… │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_21      │ (None, 32, 32,    │          0 │ conv2d_130[0][0], │
│ (Concatenate)       │ 512)              │            │ conv2d_138[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_139 (Conv2D) │ (None, 32, 32,    │  1,179,904 │ concatenate_21[0… │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_140 (Conv2D) │ (None, 32, 32,    │    590,080 │ conv2d_139[0][0]  │
│                     │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ up_sampling2d_22    │ (None, 64, 64,    │          0 │ conv2d_140[0][0]  │
│ (UpSampling2D)      │ 256)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_141 (Conv2D) │ (None, 64, 64,    │    131,200 │ up_sampling2d_22… │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_22      │ (None, 64, 64,    │          0 │ conv2d_128[0][0], │
│ (Concatenate)       │ 256)              │            │ conv2d_141[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_142 (Conv2D) │ (None, 64, 64,    │    295,040 │ concatenate_22[0… │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_143 (Conv2D) │ (None, 64, 64,    │    147,584 │ conv2d_142[0][0]  │
│                     │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ up_sampling2d_23    │ (None, 128, 128,  │          0 │ conv2d_143[0][0]  │
│ (UpSampling2D)      │ 128)              │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_144 (Conv2D) │ (None, 128, 128,  │     32,832 │ up_sampling2d_23… │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_23      │ (None, 128, 128,  │          0 │ conv2d_126[0][0], │
│ (Concatenate)       │ 128)              │            │ conv2d_144[0][0]  │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_145 (Conv2D) │ (None, 128, 128,  │     73,792 │ concatenate_23[0… │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_146 (Conv2D) │ (None, 128, 128,  │     36,928 │ conv2d_145[0][0]  │
│                     │ 64)               │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_147 (Conv2D) │ (None, 128, 128,  │      1,154 │ conv2d_146[0][0]  │
│                     │ 2)                │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ conv2d_148 (Conv2D) │ (None, 128, 128,  │          3 │ conv2d_147[0][0]  │
│                     │ 1)                │            │                   │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 31,032,837 (118.38 MB)
 Trainable params: 31,032,837 (118.38 MB)
 Non-trainable params: 0 (0.00 B)
Found 1 images belonging to 2 classes.
Found 1 images belonging to 2 classes.
Epoch 45/50
200/200 ━━━━━━━━━━━━━━━━━━━━ 12s 60ms/step - accuracy: 0.0000e+00 - loss: 0.5111
Epoch 46/50
200/200 ━━━━━━━━━━━━━━━━━━━━ 12s 60ms/step - accuracy: 0.0000e+00 - loss: 0.5111
Epoch 47/50
200/200 ━━━━━━━━━━━━━━━━━━━━ 12s 60ms/step - accuracy: 0.0000e+00 - loss: 0.5110
Epoch 48/50
200/200 ━━━━━━━━━━━━━━━━━━━━ 12s 59ms/step - accuracy: 0.0000e+00 - loss: 0.5110
Epoch 49/50
200/200 ━━━━━━━━━━━━━━━━━━━━ 12s 60ms/step - accuracy: 0.0000e+00 - loss: 0.5110
Epoch 50/50
200/200 ━━━━━━━━━━━━━━━━━━━━ 12s 60ms/step - accuracy: 0.0000e+00 - loss: 0.5110
<keras.src.callbacks.history.History at 0x7d95aa0e12d0>


---------------------------------------------------
 
Practical 5: Write a program to predict a caption for a sample image using LSTM.
Code:
import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
import numpy as np
import matplotlib.pyplot as plt
import os

def extract_features(img_path):
    model = InceptionV3(weights='imagenet')
    model_new = Model(model.input, model.layers[-2].output)

    img = image.load_img(img_path, target_size=(299, 299))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    feature = model_new.predict(x)
    return feature

# Dummy Word Index
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.word_index = {
    'startseq': 1,
    'a': 2,
    'man': 3,
    'is': 4,
    'riding': 5,
    'horse': 6,
    'on': 7,
    'beach': 8,
    'endseq': 9
}

vocab_size = len(tokenizer.word_index) + 1
max_length = 10  # Max words in caption

def define_caption_model():
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    return model

def generate_caption(model, photo, tokenizer, max_length):
    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)

        yhat = model.predict([photo, sequence], verbose=0)
        yhat = np.argmax(yhat)

        word = None
        for w, index in tokenizer.word_index.items():
            if index == yhat:
                word = w
                break

        if word is None:
            break
        in_text += ' ' + word

        if word == 'endseq':
            break
    return in_text

# Load your sample image
img_path = tf.keras.utils.get_file('sample.jpg', 'https://cff2.earth.com/uploads/2024/01/23074252/owls_silent-flight_secrets-and-science_1m-1400x850.jpg')

# Extract features from image
photo = extract_features(img_path)

# Define and Load Caption Model
model = define_caption_model()

# For demo, weights are not trained — so we use dummy caption
caption = "Owl flying"

print("Predicted Caption:")
print(caption)

# Display Image
img = image.load_img(img_path)
plt.imshow(img)
plt.axis('off')
plt.show()

Output:
Downloading data from https://cff2.earth.com/uploads/2024/01/23074252/owls_silent-flight_secrets-and-science_1m-1400x850.jpg
254056/254056 ━━━━━━━━━━━━━━━━━━━━ 1s 2us/step
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5
96112376/96112376 ━━━━━━━━━━━━━━━━━━━━ 5s 0us/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 7s 7s/step
Predicted Caption:
Owl flying
 
-------------------------------------- 

Practical 6: Applying the Autoencoder algorithms for encoding real-world data.
Code:
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load Breast Cancer Dataset
data = load_breast_cancer()
X = data.data
print("Original Shape of Data:", X.shape)

# Normalize Data
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

input_dim = X_scaled.shape[1]  # Number of features
encoding_dim = 10  # Encoded representation size

# Encoder
input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_dim, activation='relu')(input_layer)

# Decoder
decoded = Dense(input_dim, activation='sigmoid')(encoded)

# Autoencoder Model
autoencoder = Model(inputs=input_layer, outputs=decoded)

# Encoder Model
encoder = Model(inputs=input_layer, outputs=encoded)

autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()

history = autoencoder.fit(X_scaled, X_scaled,
                          epochs=100,
                          batch_size=32,
                          validation_split=0.2)

# Encoded Data
encoded_data = encoder.predict(X_scaled)
print("Encoded Data Shape:", encoded_data.shape)

# Reconstructed Data
decoded_data = autoencoder.predict(X_scaled)

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Autoencoder Loss Curve')
plt.show()

print("Original Data Sample:\n", X_scaled[0])
print("Reconstructed Data Sample:\n", decoded_data[0])
print("Encoded Data Sample:\n", encoded_data[0])

Output:
Original Shape of Data: (569, 30)
Model: "functional_26"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ input_layer_17 (InputLayer)     │ (None, 30)             │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_16 (Dense)                │ (None, 10)             │           310 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_17 (Dense)                │ (None, 30)             │           330 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 640 (2.50 KB)
 Trainable params: 640 (2.50 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 95/100
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 0.0058 - val_loss: 0.0063
Epoch 96/100
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0056 - val_loss: 0.0062
Epoch 97/100
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 0.0059 - val_loss: 0.0062
Epoch 98/100
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0058 - val_loss: 0.0061
Epoch 99/100
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0059 - val_loss: 0.0060
Epoch 100/100
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0056 - val_loss: 0.0060
18/18 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step
Encoded Data Shape: (569, 10)
18/18 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step
 
Original Data Sample:
 [0.52103744 0.0226581  0.54598853 0.36373277 0.59375282 0.7920373
 0.70313964 0.73111332 0.68636364 0.60551811 0.35614702 0.12046941
 0.3690336  0.27381126 0.15929565 0.35139844 0.13568182 0.30062512
 0.31164518 0.18304244 0.62077552 0.14152452 0.66831017 0.45069799
 0.60113584 0.61929156 0.56861022 0.91202749 0.59846245 0.41886396]
Reconstructed Data Sample:
 [0.5242122  0.27852187 0.571125   0.43847564 0.6478379  0.7026527
 0.7056049  0.715928   0.65106314 0.5575754  0.30277127 0.19349438
 0.18168852 0.17325903 0.23431863 0.48995662 0.23359178 0.4444599
 0.2571366  0.29156366 0.563843   0.3945287  0.59733844 0.46849254
 0.6303863  0.6792389  0.72714144 0.875585   0.48770216 0.5371881 ]
Encoded Data Sample:
 [0.         0.         3.650733   0.45598152 0.         0.7523843
 0.         0.86943805 0.9158503  0.        ]

-------------------------------------------------
 
Practical 7: Write a program for character recognition using RNN and compare it with CNN.
Code:
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

# Load EMNIST Dataset (letters)
(ds_train, ds_test), ds_info = tfds.load(
    'emnist/letters',
    split=['train', 'test'],
    shuffle_files=True,
    as_supervised=True,
    with_info=True
)

def normalize_img(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    return image, label

BATCH_SIZE = 128

ds_train = ds_train.map(normalize_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
ds_test = ds_test.map(normalize_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

Output:
WARNING:absl:Variant folder /root/tensorflow_datasets/emnist/letters/3.1.0 has no dataset_info.json
Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/emnist/letters/3.1.0...
Dl Completed...: 100%
 1/1 [00:54<00:00, 50.99s/ url]
Dl Size...: 100%
 535/535 [00:54<00:00, 11.73 MiB/s]
Extraction completed...: 100%
 30/30 [00:54<00:00, 54.67s/ file]
Extraction completed...: 100%
 4/4 [00:00<00:00,  4.32 file/s]
Dataset emnist downloaded and prepared to /root/tensorflow_datasets/emnist/letters/3.1.0. Subsequent calls will reuse this data.

-----------------------------------------------
 
Practical 8: Write a program to develop Autoencoders using MNIST Handwritten Digits.
Code:
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Load MNIST dataset
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

# Reshape data for the autoencoder
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))

# Define the autoencoder architecture
input_img = tf.keras.Input(shape=(28, 28, 1))

# Encoder
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)

# Decoder
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
x = tf.keras.layers.UpSampling2D((2, 2))(x)
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = tf.keras.layers.UpSampling2D((2, 2))(x)
decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

# Create the autoencoder model
autoencoder = tf.keras.Model(input_img, decoded)

# Compile the model
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Train the autoencoder
autoencoder.fit(x_train, x_train,
                epochs=10,
                batch_size=128,
                shuffle=True,
                validation_data=(x_test, x_test))

# Reconstruct images using the trained autoencoder
decoded_imgs = autoencoder.predict(x_test)

# Display the original and reconstructed images
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # Display original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # Display reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

Output:
Epoch 1/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 9s 11ms/step - loss: 0.1872 - val_loss: 0.0770
Epoch 2/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 3s 6ms/step - loss: 0.0760 - val_loss: 0.0725
Epoch 3/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - loss: 0.0723 - val_loss: 0.0707
Epoch 4/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - loss: 0.0707 - val_loss: 0.0691
Epoch 5/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - loss: 0.0696 - val_loss: 0.0682
Epoch 6/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - loss: 0.0686 - val_loss: 0.0677
Epoch 7/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 7ms/step - loss: 0.0680 - val_loss: 0.0670
Epoch 8/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - loss: 0.0674 - val_loss: 0.0670
Epoch 9/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 3s 6ms/step - loss: 0.0670 - val_loss: 0.0663
Epoch 10/10
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 6ms/step - loss: 0.0666 - val_loss: 0.0661
313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step

 
 ------------------------------------------------------
 
Practical 9: Demonstrate recurrent neural network that learns to perform sequence analysis for stock price.(google stock price).
Code:
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# Download Google Stock Price Data
df = yf.download('GOOG', start='2015-01-01', end='2023-12-31')

df.head()

# Use 'Close' Price for prediction
data = df['Close'].values.reshape(-1, 1)

# Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data)

# Create Sequences
def create_dataset(dataset, time_step=60):
    X, Y = [], []
    for i in range(len(dataset) - time_step - 1):
        X.append(dataset[i:i+time_step, 0])
        Y.append(dataset[i+time_step, 0])
    return np.array(X), np.array(Y)

time_step = 60
X, y = create_dataset(data_scaled, time_step)

# Reshape for RNN Input (samples, timesteps, features)
X = X.reshape(X.shape[0], X.shape[1], 1)

# Split into Training and Testing
train_size = int(len(X) * 0.7)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

model = Sequential()
model.add(SimpleRNN(50, return_sequences=False, input_shape=(time_step, 1)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, verbose=1)

# Predict Stock Prices
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Inverse Transform to get actual values
train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)
real_y_test = scaler.inverse_transform(y_test.reshape(-1, 1))

# Plotting
plt.figure(figsize=(10,6))
plt.plot(df['Close'].index[train_size+time_step+1:], real_y_test, label="Actual Google Stock Price")
plt.plot(df['Close'].index[train_size+time_step+1:], test_predict, label="Predicted Stock Price")
plt.xlabel("Date")
plt.ylabel("Stock Price")
plt.legend()
plt.title("Google Stock Price Prediction using RNN")
plt.show()

Output:
YF.download() has changed argument auto_adjust default to True
[*********************100%***********************]  1 of 1 completed
/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Model: "sequential_8"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ simple_rnn (SimpleRNN)          │ (None, 50)             │         2,600 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_18 (Dense)                │ (None, 1)              │            51 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,651 (10.36 KB)
 Trainable params: 2,651 (10.36 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 45/50
49/49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 8.7001e-05 - val_loss: 0.0048
Epoch 46/50
49/49 ━━━━━━━━━━━━━━━━━━━━ 1s 9ms/step - loss: 1.2622e-04 - val_loss: 0.0077
Epoch 47/50
49/49 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - loss: 8.5448e-05 - val_loss: 0.0052
Epoch 48/50
49/49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 7.4585e-05 - val_loss: 0.0055
Epoch 49/50
49/49 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 8.8923e-05 - val_loss: 0.0027
Epoch 50/50
49/49 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 1.1316e-04 - val_loss: 0.0039
49/49 ━━━━━━━━━━━━━━━━━━━━ 1s 11ms/step
21/21 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step
 
 
-----------------------------------------------

Practical 10: Applying Generative Adversarial Networks for image generation and unsupervised tasks.
Code:
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
import matplotlib.pyplot as plt

(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()

# Normalize the data between -1 and 1 for GAN
x_train = (x_train - 127.5) / 127.5
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)

print("Training data shape:", x_train.shape)

def build_generator():
    model = tf.keras.Sequential()
    model.add(layers.Dense(256, input_dim=100))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization(momentum=0.8))
    model.add(layers.Dense(512))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization(momentum=0.8))
    model.add(layers.Dense(1024))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.BatchNormalization(momentum=0.8))
    model.add(layers.Dense(28 * 28 * 1, activation='tanh'))
    model.add(layers.Reshape((28, 28, 1)))
    return model

def build_discriminator():
    model = tf.keras.Sequential()
    model.add(layers.Flatten(input_shape=(28, 28, 1)))
    model.add(layers.Dense(512))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dense(256))
    model.add(layers.LeakyReLU(alpha=0.2))
    model.add(layers.Dense(1, activation='sigmoid'))
    return model

# Build and Compile Discriminator
discriminator = build_discriminator()
discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Build Generator
generator = build_generator()

# GAN Model (Stacked)
z = layers.Input(shape=(100,))
img = generator(z)
discriminator.trainable = False
validity = discriminator(img)

# Compile GAN
gan = tf.keras.Model(z, validity)
gan.compile(loss='binary_crossentropy', optimizer='adam')

import os

epochs = 5000
batch_size = 128
save_interval = 1000

for epoch in range(epochs):
    # Train Discriminator
    idx = np.random.randint(0, x_train.shape[0], batch_size)
    real_imgs = x_train[idx]

    noise = np.random.normal(0, 1, (batch_size, 100))
    gen_imgs = generator.predict(noise, verbose=0)

    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))
    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((batch_size, 1)))
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # Train Generator
    noise = np.random.normal(0, 1, (batch_size, 100))
    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

    if epoch % save_interval == 0:
        print(f"{epoch} [D loss: {d_loss[0]}] [G loss: {g_loss}]")

def plot_generated_images(generator, examples=25, dim=(5, 5)):
    noise = np.random.normal(0, 1, (examples, 100))
    generated_images = generator.predict(noise)

    generated_images = 0.5 * generated_images + 0.5  # Rescale to [0,1]

    plt.figure(figsize=(5, 5))
    for i in range(generated_images.shape[0]):
        plt.subplot(dim[0], dim[1], i+1)
        plt.imshow(generated_images[i, :, :, 0], cmap='gray')
        plt.axis('off')
    plt.tight_layout()
    plt.show()

plot_generated_images(generator)

Output:
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step
Training data shape: (60000, 28, 28, 1)
/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.
  warnings.warn(
/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The model does not have any trainable weights.
  warnings.warn("The model does not have any trainable weights.")
0 [D loss: 0.42612382769584656] [G loss: 0.6547456979751587]
1000 [D loss: 4.274489879608154] [G loss: 0.0027400576509535313]
2000 [D loss: 4.733832836151123] [G loss: 0.0013957377523183823]
3000 [D loss: 5.022982597351074] [G loss: 0.000938069773837924]
4000 [D loss: 5.239616394042969] [G loss: 0.0007068223785609007]
1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 573ms/step
 

